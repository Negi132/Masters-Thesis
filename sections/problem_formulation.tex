\section{Problem Formulation \& 
Prerequisite} 
\label{sec:problem-formulation-and-approach}
This paper aims to help \gls{aml} 
employees focus their efforts on the 
alarms that matter. To do this, we must address the underlying problem that 
current rules-based AML systems generate a large volume of false-positive 
alarms, creating a significant operational burden while still requiring 
near-perfect detection of true fraud due to regulatory constraints.
Given this setting, the challenge is not to replace existing alarm systems, but 
to reduce analyst workload without discarding any alarms outright. 

To address this problem, the paper investigates whether different modelling 
assumptions and data partitioning strategies affect the ability to classify 
pre-raised alarms under real-world constraints. We will investigate:

\begin{itemize}
	\item Whether a monolithic classification model (i.e., a single classifier
	trained on the complete, non-partitioned dataset, as opposed to ensemble
	approaches that divide data into subsets) is sufficient for classifying
	pre-raised AML alarms.
	\item Whether partitioning data by AML employee captures decision-making 
	patterns that improve classification.
	\item Whether alarm categoryâ€“specific models better capture heterogeneity 
	in alarm types. Alarm categories are described in section 
	\ref{sec:problemFormulationAMLWorkflow}.
	\item Whether more expressive models, such a tabular oriented \glspl{nn}, 
	provide benefits under severe class imbalance and limited data.
	\item Whether combining heterogeneous classifiers into a voting classifier 
	yields more robust prioritisation.
	\item Whether an automated ML pipelines, such as AutoGluon, a 
	state-of-the-art automatic \gls{ml} library that automates the \gls{ml} 
	process, can outperform manually configured models in this constrained 
	setting.
\end{itemize}
Each model will 
be tested with multiple different 
permutations of 
base-classifier, to ensure thorough analysis of the viability of simplistic 
classification algorithms for alleviation 
of \gls{aml} employee workload. We will 
also investigate the performance of 
multiple \gls{nn} classifiers, even though 
we suspect their performance will be 
lacking, due to the nature and amounts of 
data available.

\subsection{The AML Workflow}\label{sec:problemFormulationAMLWorkflow}
To contextualize the problem and its operational constraints, we first describe 
the \gls{aml} workflow within \gls{lsb}. A sketch of their 
general workflow, as well as the suggested changes from this project, can be seen in \autoref{fig:amlFlow}.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/aml_clas_flow.png}
	\caption{Proposed workflow of 
		\gls{aml} employees after implementing 
		our model. The model acts as a 
		screening device, allowing for 
		employees to focus on the alarms that 
		are most likely to be true alarms. Blue nodes represent the existing bank workflow. Tan-coloured nodes are changed 
		or added by implementing the model.}
	\label{fig:amlFlow}
\end{figure*}

Within \gls{lsb} it is the work of \gls{aml} 
employees to 
distinguish between real and false alarms 
flagged by their 
system. They have a rules-based system 
with approximately 70 different flags 
that a transaction can trigger. These 
flags are categorized into nine 
categories: 

\begin{multicols}{2}
	\begin{enumerate}
		\item Tax haven countries
		\item Deviation from \\expected behaviour
		\item Risk list entities
		\item Watch list entities
		\item Debitcard transactions
		\item Wire activity
		\item ATM/Phone activity
		\item Manual alarms
		\item Counterparty terror
	\end{enumerate}
\end{multicols}

Of these categories, the only one that is 
discarded when training our models is 8 - 
Manual Alarms, as they are not raised by the 
system, but are instead raised 
manually by employees. These alarms are created based on reasons that
come from outside the system itself (e.g. phone 
calls, meetings, etc.) Therefore, it 
would be impossible for 
our system to classify them. \\
When a flag is triggered, an alarm is made 
to the system, 
detailing everything from what, how, when, 
and who raised suspicion. Furthermore, the category, 
account, and person who triggered the 
alarm are also taken into consideration. It is then up to 
each AML employee to investigate what type 
of alarm it is, how 
it was triggered, if it is consistent with 
account patterns, and 
to potentially ask for additional 
documentation from the account 
holder. As can be seen in \autoref{fig:amlFlow}, if an employee deems 
that additional information is required the label of the alarm may 
change multiple times as information becomes available. If no 
additional information is required, the alarm will be closed without 
ever contacting the customer, before the looping node. The end result 
of this 
investigation will then be either 
a \textit{CLOSED} or a \textit{ADDED TO 
CASE} label, henceforth referred to simply as \textit{ADDED}. The 
\textit{CLOSED} label means that the alarm 
was a false positive, 
and the \textit{ADDED} label means 
that the bank cannot 
rule out fraud. When the \textit{ADDED} label is given to an alarm it 
is then, along with any evidence or documentation gathered by the 
\gls{aml} employee, sent to the 
appropriate authorities. A third label 
\textit{ACTIVE} can also 
be given to an alarm but this label 
is always temporary, 
as this is the label for an ongoing 
investigation. In this paper, 
we discard all instances of this label, as 
we cannot be certain 
of its proper label yet. Lastly, if the investigation of an alarm 
cannot be closed by an \gls{aml} employee without additional 
information or actions, the alarm may be assigned a variety of 
interim-labels, indicating its current status. These labels are 
unimportant for the models, which is further described in 
\ref{subsec:prerocesing_labels}.\\
In summary, the problem addressed in this paper is whether pre-raised \gls{aml} 
alarms can be reliably prioritized using post-alarm classification models, 
under extreme class imbalance, limited data availability, and strict regulatory 
constraints that prohibit automatic dismissal of alarms. It should be mentioned that we will not assign alarms a probability of fraud and 
will only classify them in a binary manner. This classification is what is 
meant by 'high' and 'low' priority alarms. The 'high' priority alarms being the 
ones that the classifier assigns the \textit{ADDED} label, i.e. the ones where 
\gls{lsb}'s rule-system and the classification system agree, and the 'low' 
priority alarms being the ones where the two systems disagree.
