\section{Results}\label{sec:results}
More results can be seen in \Cref{app:B}, however for the sake 
of comparison and readability, summary tables have been used in the following sections.
\subsection{Monolith vs. Ensembles}
% 25 - cat, 23 - employee
% mono - 0
The monolithic approach demonstrates the most balanced performance, securing the highest scores in \textit{Precision Added}, \textit{Accuracy}, and \textit{FBeta}, as seen in~\Cref{tab:ensemble_results}. When it comes for individual metrics, the results are balanced between the different approaches. Among the base classifiers, \gls{rfc} was the most volatile, appearing as both a top-tier performer (e.g., in \textit{Precision Closed}) and a bottom-tier performer (e.g., in \textit{Recall Closed} and \textit{Accuracy}). This mostly depends on the eight optimization criteria.

The results within the ensembles follow a similar pattern. However, the values reach much more extreme polarities, as shown in \Cref{tab:ensemble_results}. This means that ensembles more often reach near-perfect scores in one metric while also performing with near-zero scores in others. For instance, the \gls{rfc} with \textit{rec\_add} scorer reaching $1.000$ in \textit{Recall Added} but only $0.002$ in \textit{Recall Closed}. 

This is further reinforced by comparing the group mean values per approach. As we can see in Table \ref{tab:group_mean} mono achieves higher mean. If we isolate the ranking to only viable models then the difference become much lower as seen in Table  \ref{tab:group_mean_viable}, but still if we take the top 15 models, 11 are using the mono approach. Categorizing a model as viable is further explained in \ref{sec:useful_models}.

\begin{table}[H]
	\centering
	\begin{tabular}{l r r}
		\toprule
		Approach & Mean Value & n \\
		\midrule
		Mono              & 0.5205 & 64 \\
		Category Ensemble & 0.3626 & 64 \\
		Employee Ensemble & 0.2880 & 64 \\
		\bottomrule
	\end{tabular}
	\caption{Ranking of approach by mean fbeta}
	\label{tab:group_mean}
\end{table}

\begin{table}[h]

	\centering
	\begin{tabular}{l r r}
		\toprule
		Approach & Mean Value & n \\
		\midrule
		Mono              & 0.6968 & 18 \\
		Employee Ensemble & 0.6778 & 7  \\
		Category Ensemble & 0.6637 & 4  \\
		\bottomrule
	\end{tabular}
	\caption{Ranking of approach by mean fbeta on viable models}
	\label{tab:group_mean_viable}
\end{table}

This suggests model \textit{over-optimization} which is further discussed in the following chapter. As shown in \Cref{tab:collapsed_rates}, both the category ensemble ($25.0\%$) and employee ensemble ($23.4\%$) are prone to rates of over-optimization. This suggests the monolith approach is more reliable for maintaining balanced classification performance.

\subsection{Over-optimized Models}\label{sec:collapsed}
During model selection and hyperparameter optimization, we can observe that
a subset of runs converged to extreme highs and extreme lows for certain metrics. Such models have learned a prediction
pattern that is dominated by a single class (or produces scores that yield
near-zero discriminative results), which is very problematic under class
imbalance as a model can appear competitive on aggregate metrics such as accuracy. Over-optimized models predict the majority class, while failing to identify the minority class
of interest.

We can identify an over-optimized models if any of the following conditions hold:
\begin{itemize}
	\item \textbf{Predicts almost all negatives:}
	$\recall_{\mathrm{pos}} \le 0.01$ and $\recall_{\mathrm{neg}} \ge 0.95$
	\item \textbf{Predicts almost all positives:}
	$\recall_{\mathrm{neg}} \le 0.01$ and $\recall_{\mathrm{pos}} \ge 0.95$
	\item \textbf{High accuracy but finds virtually no positives:}
	$\accuracy \ge 0.83$ and $\recall_{\mathrm{pos}} \le 0.01$
	\item \textbf{Near-zero positive precision:}
	$\precision_{\mathrm{pos}} \le 0.05$
	\item \textbf{Near-zero F1 score:}
	$F_{1} \le 0.05$
\end{itemize}


These criteria are are not intended as a statistical
test of performance, but as practical guardrails to identify models whose
behaviour is dominated by a single class or whose positive-class performance is
so poor that the model is effectively unusable. We therefore
report \emph{over-optimized rates} (Table~\ref{tab:collapsed_rates}) as the fraction
of trained models within each approach that triggered at least one of the above
conditions.


\begin{table}[t]
	\centering
	\begin{tabular}{l r r r}
		\toprule
		Approach & \makecell{Over-Optimized Rate\\(Fraction)} & \makecell{Over-optimized\\Count} & \makecell{Total\\Models} \\
		\midrule
		Category Ensemble & 0.250 (16/64) & 16 & 64 \\
		Employee Ensemble & 0.234 (15/64) & 15 & 64 \\
		Mono & 0.000 (0/64) & 0 & 64 \\
		\bottomrule
	\end{tabular}
	\caption{Over-optimized Rates Across Approaches}
	\label{tab:collapsed_rates}
\end{table}

\subsection{Viable vs.\ Non-viable Non-overoptimized Models}\label{sec:useful_models}
While the over-optimized criteria in \Cref{sec:collapsed} remove runs that are
effectively dominated by a single class, many remaining models
still fail to meet minimum operational requirements. To make this distinction
explicit, we further partition \emph{non-overoptimized} models into two groups based
on recall constraints that reflect the intended prioritization use-case.

\textbf{Viable models} are defined as the \emph{non-overoptimized} runs that satisfy
both recall thresholds:
\[
\texttt{recall}_{\texttt{added}} \ge 0.90
\quad \land \quad
\texttt{recall}_{\texttt{closed}} \ge 0.10.
\]

\begin{table}[t]
	\centering
	\begin{tabular}{l r r r}
		\toprule
		Approach & \makecell{Viable Rate\\(Fraction)} & Viable Count & Non-overoptimized \\
		\midrule
		Category Ensemble & 0.083 (4/48) & 4 & 48 \\
		Employee Ensemble & 0.143 (7/49) & 7 & 49 \\
		Mono & 0.281 (18/64) & 18 & 64 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of Viable Rates Across Approaches}
	\label{tab:useful_non_useful}
\end{table}

\textbf{Non-viable models} are the remaining \emph{non-overoptimized} runs that miss
at least one of these thresholds. Over-optimized models are excluded from both
categories.

The asymmetry of these thresholds is deliberate. The primary requirement is
high recall on \textit{ADDED}, since false negatives are
operationally costly. The weaker constraint on \textit{CLOSED} ensures the model
retains at least minimal discriminative ability on non-escalations, preventing a
trivial ``flag-everything'' regime from being treated as operationally
acceptable.

The resulting Viable rates are shown in \Cref{tab:useful_non_useful}. In absolute
terms, the monolithic approach yields the largest pool of viable configurations
(18/64), whereas the category and employee ensembles yield substantially fewer
viable configurations (4/64 and 7/64, respectively). Put differently, even after
excluding over-optimized runs, most trained models are still non-viable under these
recall constraints (Category Ensemble: 44/48 non-overoptimized, Employee Ensemble:
42/49; Mono: 49/64). This indicates that simultaneously achieving very high
escalation recall while retaining non-trivial recall on closed alarms is a
relatively stringent requirement, and it is satisfied most consistently by the
monolithic training regime.

\input{fig/mono_results.tex}

\input{fig/ensemble_results.tex}

After analyzing the results, we can go trough the models and isolated the better performing ones based on certain criteria. This helps us separate models that do not perform well or being over-optimized during training. Over-optimized models try to satisfy the given loss function and sacrifices other metrics. For example if a model is optimized for recall on the positive label it might achieve perfect score there and a really bad score on the other metrics. Additionally, due to the imbalanced data, such models can still achieve good accuracy simply due to always predicting the majority class. % <list criteria>. and reference over-optimisation

\input{fig/regular_results.tex}


\subsection{Neural Network Results}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/bce_thresholds.pdf}
    \caption{Threshold sensitivity for RNN, GRU, and Transformer under binary cross-entropy loss. Each subplot shows recall (ADDED/CLOSED), F1-score, and accuracy across thresholds; the dashed line marks the standard $0.5$ operating point.}
    \label{fig:bce_thresholds}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/focal_thresholds.pdf}
    \caption{Threshold sensitivity for RNN, GRU, and Transformer under focal loss. All architectures exhibit recall-dominant regimes at low thresholds with limited balanced operating regions. The dashed line marks the $0.5$ threshold.}
    \label{fig:focal_thresholds}
\end{figure}

Figures \ref{fig:bce_thresholds} and \ref{fig:focal_thresholds}
showcase a summary of the results of the training of the neural networks.

For the \gls{rnn} trained with \gls{bce}
(Table~\ref{tab:bce_loss_rnn_new}),
the best trade-off between precision and recall for escalated
cases is obtained at a threshold of 0.5. At this operating point,
the model achieves a balanced F1 score while maintaining non-trivial recall
for both escalated ($\texttt{recall}_{\texttt{added}} \approx 0.52$)
and closed ($\texttt{recall}_{\texttt{closed}} \approx 0.54$) alarms.
This indicates that \gls{bce} yields a relatively smooth score distribution,
enabling balanced operating points that simultaneously capture escalations and
avoid excessive false positives.

\input{fig/rnn_results_bce_new.tex}
In contrast, the \gls{rnn} trained with focal loss (Table
\ref{tab:focal_loss_rnn_new})
exhibits a strongly recall-dominated regime. At thresholds up to $0.35$,
the model achieves near-perfect recall for escalated cases
($\texttt{recall}_{\texttt{added}} \geq 0.925$),
indicating that nearly all escalations are ranked above the decision threshold. 
However, this behaviour
is accompanied by near-zero recall for closed cases and low overall accuracy,
reflecting a deliberate prioritization of false-negative avoidance over balanced
discrimination.

\input{fig/rnn_results_focal_new.tex}

Compared to the \gls{rnn}, the \gls{gru} exhibits improved discrimination
between
escalated and closed alarms, yielding higher precision and recall for escalated
cases at an operating point of $0.5$ (Table~\ref{tab:bce_loss_gru_new}).
This suggests that the \gls{gru}'s gating mechanisms improve the aggregation of
temporal risk signals, resulting in more informative sequence representations.

\input{fig/gru_results_bce_new.tex}

When trained with focal loss (Table~\ref{tab:focal_loss_gru_new}),
the \gls{gru} exhibits a pronounced recall-oriented operation, once again
achieving high recall for escalated cases at low thresholds 
($\texttt{recall}_{\texttt{added}} \approx 0.925$ at threshold $0.25$). 
Similarly,
this behaviour is accompanied by poor recall for closed cases and limited
flexibility
at intermediate thresholds. While the \gls{gru} moderates the extreme score
polarization,
focal loss remains ill-suited for balanced operating points in this setting.

\input{fig/gru_results_focal_new.tex}

For the Transformer trained with binary cross-entropy
(Table~\ref{tab:bce_loss_transformer_new}),
the results reveal a pronounced failure mode: at thresholds below $0.5$,
the model achieves perfect recall for escalated cases
($\texttt{recall}_{\texttt{added}} = 1.0$) but with zero recall for closed cases,
while at thresholds $\geq 0.55$ the pattern reverses completely with zero recall
for escalated cases and perfect recall for closed cases. This extreme
polarization suggests that the Transformer's score distribution has collapsed,
with predictions clustered around a narrow threshold band.
At $0.5$, the model achieves moderate recall for both classes
($\texttt{recall}_{\texttt{added}} \approx 0.49$,
$\texttt{recall}_{\texttt{closed}} \approx 0.59$),
but this balanced regime is fragile and disappears with small threshold shifts.
Unlike the recurrent architectures, the Transformer
fails to maintain stable operating regimes under \gls{bce}, highlighting
architecture-specific
weaknesses in probability calibration.

\input{fig/transformer_results_bce_new.tex}

Under focal loss (Table~\ref{tab:focal_loss_transformer_new}),
the Transformer exhibits a recall-dominated regime similar to the recurrent
models, achieving perfect recall for escalated cases at threshold $0.4$
($\texttt{recall}_{\texttt{added}} = 1.0$).
While self-attention slightly moderates the collapse observed in recurrent
architectures, focal loss still yields limited flexibility at intermediate
operating points, reinforcing its suitability for recall-critical rather than
balanced settings.
\input{fig/transformer_results_focal_new.tex}

Across the recurrent architectures (\gls{rnn} and \gls{gru}), \gls{bce}
consistently supports
balanced operating points near a threshold of 0.5, where both recall for
escalated and closed cases remain non-trivial. However, the
Transformer exhibits a failure mode under \gls{bce}, suggesting that
self-attention mechanisms
require careful calibration to maintain stable probability distributions. In
contrast, focal loss enforces recall-dominant regimes regardless of model
capacity, indicating that loss choice has a stronger influence on decision
geometry than architectural complexity in recurrent models, while the
Transformer demonstrates
architecture-specific weaknesses that transcend loss function selection.

\subsection{Voting Classifier Results}
\label{results_voting}
The performance of the \gls{vc} is presented in \Cref{tab:voting_reg}. It differs from the individual models as it is more balanced in terms of results with lower maximums but higher minimums across the metrics.\\ 
The prior over-optimization seen in \gls{mm}, \gls{ce}, and \gls{ee} approaches is not present here due to the added diversity when combining models. By combining predictions, the \gls{vc} filters out the extreme results, leading to a more stable performance in general.\\
However, despite this increased consistency, the \gls{vc} showed lower results in Accuracy and F-beta, compared to the viable models found in the other methods.

\subsection{AutoGluon Results}
AutoGluon achieves the strongest performance among all evaluated
approaches, as shown in \Cref{tab:autogluon_per_model}.


\subsection{Summary of Findings}
The experimental evaluation conducted in this study examines traditional machine
learning classifiers, ensemble methodologies, voting classifiers, and neural
network architectures. Among the traditional classifiers, monolithic models
outperform both employee-based and category-based ensembles, with the
LogisticRegression optimized for fbeta on the \textit{ADD} label
achieving an accuracy of $52.3\%$ with overall balanced metrics and an f-beta score of $72.6\%$
(see~\Cref{tab:nonoptuna}).

The advantage of monolithic models relative to ensemble approaches 
can be attributed to the limited size of the available dataset, which renders 
further partitioning inefficient and prevents ensemble components from learning
sufficiently robust patterns. The ensemble hypothesis, which is motivated by
the conjecture that human decision making patterns may exhibit themselves 
within data partitioned per employee and thereby yield performance gains, does 
not materialize in practice. This outcome suggests that the data volume is 
insufficient to support such fine-grained specialization. This conclusion is
reinforced by the over-optimization analysis, where employee-based and
category-based ensembles contain more over-optimized models models than the
monolithic approach, indicating weaker generalization under the same protocol.

The neural network experiments show that deep learning approaches achieve
comparable performance to traditional machine learning methods on this limited
dataset. Under binary cross-entropy loss at threshold $0.5$, the best-performing
neural architecture (\gls{gru}) achieves an accuracy of approximately $55\%$ and
an F1 score of $0.27$, which is similar to the best traditional classifier
(LogisticRegression at $52.3\%$ accuracy). Focal loss does not improve overall
performance: while it achieves high recall for escalated cases (e.g., $92.5\%$
for the \gls{gru} at threshold $0.25$), this comes at the expense of low recall
for closed cases, yielding accuracy below $25\%$ and F1 scores of approximately
$0.28$. The modest accuracy achieved by all neural architectures suggests that
the limited dataset size constrains the ability of deep learning methods to
learn robust representations.

Among the neural architectures, binary cross-entropy loss facilitates balanced
operating points for recurrent models (\gls{rnn} and \gls{gru}), where both
recall for escalated and closed cases remain non-trivial near a threshold of
$0.5$. The \gls{gru} architecture exhibits marginally better discrimination
between escalated and closed alarms relative to the standard \gls{rnn} (F1 of
$0.276$ vs $0.265$), potentially attributable to its gating mechanisms. The
Transformer architecture exhibits unstable behavior under \gls{bce}, with
predictions clustered around a narrow threshold band: at $0.5$ it achieves
moderate recall for both classes ($\approx 0.49$ for escalated, $\approx 0.59$
for closed), but small threshold shifts cause complete collapse to single-class
prediction. Focal loss consistently induces
recall-dominant operational regimes across all architectures, achieving
near-perfect or high recall for \textit{ADDED} cases at low classification
thresholds, albeit at the expense of diminished precision and reduced
flexibility at intermediate operating points.

The voting classifier experiments demonstrate that combining different base
classifiers trained on identical data partitions can reduce extreme behaviors
and improve stability. By systematically evaluating combinations of three models
optimized for identical performance metrics, it is possible to leverage the
complementary strengths of distinct algorithms while preserving the benefits
derived from hyperparameter optimization. However, the voting classifier does
not consistently outperform the strongest monolithic models on balanced metrics,
and the best configurations can correspond to unfavorable trade-offs between
\textit{ADDED} and \textit{CLOSED} performance.

Additionally, AutoGluon achieves the strongest overall performance on this
dataset, including configurations with high accuracy and strong F-beta scores,
while also maintaining comparatively better balance across class-wise metrics
than most alternatives (see Table XV). While its results can vary across
configurations, its best models are competitive across both performance and
balance criteria.

In conclusion, the experimental results show that monolithic traditional machine
learning models outperform data-partitioned ensemble methodologies and the
evaluated neural network architectures on this dataset, and that AutoGluon
achieves the best overall results.
