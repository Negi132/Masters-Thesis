\section{Experiments} \label{sec:experiments}
This section will outline our evaluation criteria, our experimental setup, 
as well as the results of our training and testing. 

\subsection{Evaluation Criteria}\label{sec:evaluationCriteria}
Before discussing the experiments and 
their results, we must first discuss on 
what basis the models will be evaluated. 
The purpose of this paper is to assist 
\gls{aml} employees in focusing on the 
alarms that matter. For a model to do this 
in the optimal way, it must distinguish 
between true alarms and false alarms 
perfectly. 
However, the creation of a perfect classifier is an unattainable goal, so we
must look elsewhere. In operational settings, the classifier should ideally
satisfy one of two extreme requirements. The first option is to correctly
classify all \textit{ADDED} alarms, which is equivalent to achieving $100\%$
\textit{recall} on the \textit{ADDED} label. The second option is to not 
classify any \textit{ADDED} alarms incorrectly, which would be equivalent to 
achieving $100\%$ \textit{precision} on 
the \textit{CLOSED} label\footnote{While these two metrics are mathematically 
	identical in the perfect case, outside of such scenarios their practical 
	qualities differ. Proof of this can be 
	found in ~\Cref{app:A}.}. 
Definitions for these two metrics can be seen in equations \ref{eq:reca} and
\ref{eq:prec}.\\

\captionof*{equation}{EQUATIONS (1) to (5):\\ \textit{
		TP = True Positive, or correctly classified instances of the positive 
		class. \\
		TN = True Negative, or correctly classified instances of the negative 
		class. \\
		FP = False Positive, instances of the negative class that have been falsely 
		classified as positive. \\
		FN = False Negative, instances of the positive class that have been falsely 
		classified as negative.}}
\begin{equation}\label{eq:reca}
	\text{Recall} = 
	\frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\begin{equation}\label{eq:prec}
	\text{Precision} = 
	\frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}

By striving for these metrics, we would minimize the two most important error types.
Should our solution reach $100\%$ \textit{recall} on the \textit{ADDED} label, and this
performance also hold on unseen data, then no \textit{ADDED} alarms would be classified as
\textit{CLOSED} (i.e., $FN = 0$). Conversely, should our solution reach $100\%$ \textit{precision}
on the \textit{ADDED} label, and this performance also hold on unseen data, then no \textit{CLOSED}
alarms would be classified as \textit{ADDED} (i.e., $FP = 0$). \\
Since $100\%$ performance cannot be guaranteed, we report precision and recall to quantify
these trade-offs on held-out test data.\\


Judging which model might be best for this sorting process is not easy, since
achieving a high score on \textit{recall} or \textit{precision} is not enough.
There must be balance between the metrics, which is represented by the addition
of summary metrics such as \textit{accuracy}, \textit{F1-score}, and the more
general \textit{$F_{\beta}$-score}. The definitions of these metrics can be seen
in equations \ref{eq:accu}, \ref{eq:f1}, and \ref{eq:fbeta}. 

\begin{equation}\label{eq:accu}
	\text{Accuracy} = \frac{\text{TP} + 
		\text{TN}}
	{\text{TP} + \text{TN} + \text{FP} + 
		\text{FN}}
\end{equation}

\begin{equation}\label{eq:f1}
	\text{F1-score} = 2 \cdot
	\frac{\text{Precision} \cdot 
		\text{Recall}}
	{\text{Precision} + \text{Recall}}
\end{equation}

In addition to $F1$, we report the \textit{$F_{\beta}$-score}, which allows
explicit control over the trade-off between precision and recall:
\begin{equation}\label{eq:fbeta}
	 	\text{F}_{\beta}\text{-score} = (1+\beta^2)\cdot
	 	\frac{\text{Precision} \cdot \text{Recall}}
	 	{\beta^2\cdot \text{Precision} + \text{Recall}}
	 \end{equation}
When $\beta > 1$, recall is weighted more heavily than precision when
$\beta < 1$, precision is weighted more heavily.  In this study we report $F_{3}$
to reflect a strong preference for recall on the \textit{ADDED} class, while
still penalising excessive false positives. For $\beta=3$ we have $\beta^2=9$,
so
\[
F_3=\frac{(1+9)\,PR}{9P+R}=\frac{10PR}{9P+R},
\]
which can be interpreted as weighting recall nine times more than precision.

The reason for a high \textit{recall} or \textit{precision} score not being 
enough, is that a classifier 
labelling all alarms as 
\textit{ADDED}, would achieve $100\%$ 
\textit{recall} on the \textit{ADDED} label. 
However, such a classifier would not 
be useful for the purpose of lowering 
false positives, and determining 
high-priority alarms. Due to the fact that nothing could be 
designated as low-priority, since nothing 
would have been labelled \textit{CLOSED}, thereby necessitating the 
\textit{accuracy}, \textit{F1-score} metrics and \textit{$F_{\beta}$-score}. \\

To rank classifiers on their ability to 
correctly classify the alarms, we will 
evaluate them based on eight different 
criteria, with eight versions of each base 
classifier being trained, one optimized 
for each criteria. The eight 
different criteria are:
\begin{multicols}{2}
	\begin{itemize}
		\item Recall \textit{ADDED}
		\item Precision \textit{ADDED}   
		\item Accuracy 		
		\item F-Beta \textit{ADDED}
		\item Recall \textit{CLOSED}
		\item Precision \textit{CLOSED}
		\item F1-Score  
		\item F-Beta \textit{CLOSED} 
	\end{itemize}
\end{multicols}
The definitions of each metric can be seen in 
~\Cref{eq:prec,eq:reca,eq:accu,eq:f1,eq:fbeta}. Should no model exhibit clear dominance 
over others, the best will be decided by amount of high scoring metrics with 
ties broken by summation of model metric values.

\subsection{Experimental Setup}
This section provides clarification on the details of the experimental setup and 
training of all models.

\subsubsection{Train-Validation-Test Protocol}\label{sec:split_protocol}
All experiments follow the same three-way split into train,
validation, and test data to ensure comparable results across
traditional tabular models, ensemble/voting variants, and neural networks. 
This 
means 
that all \glspl{mm}, all \glspl{ee}, and all \glspl{ce} were trained on the same split. The reason for this is that later 
on in the analysis, we will be combining different base classifiers from the 
originally homogeneous ensembles into a 
voting classifier, and if they were not trained on the same split, some models may have encountered the test data in 
their own training.
Model parameters are fit on the training split. 
Across all experiments, we tune this decision rule to maximize recall on the
\textit{ADD} class on the validation split, reflecting the priority of
surfacing potentially fraudulent alarms for review. This validation-time
threshold selection is applied consistently even when the model itself was
trained or optimized using a different scoring metric. Final performance is
reported once on the held-out test split using this fixed configuration.


Where model families require stratification or partitioning (e.g., employee- or
category-specific splits for ensembles), the partitioning is applied \emph{within}
the train/validation/test framework so that no model or component is tuned on, or
exposed to, the test split.\\

\subsubsection{Cross Validation}
To obtain robust performance estimates during model selection under severe class
imbalance, we use \textit{stratified} $k$-fold cross validation on the
training split. Stratification ensures that each fold preserves the
overall \textit{CLOSED}/\textit{ADDED} class distribution. We set $k=5$ by
default and reduce it only if necessary to ensure that every fold contains at
least one instance of the minority class, with $k \ge 2$, avoiding folds with
no minority examples. For each hyperparameter setting, the model is
trained on $k-1$ folds and evaluated on the remaining fold. Scores are
aggregated across folds using the mean to rank hyperparameters settings.\\


\subsubsection{Data Imbalance}
For dealing with the imbalance present within our dataset, we utilise both 
\gls{smote} and weighting of the classes. Given the constraints of the 
available dataset and the system's role as a decision-support tool rather than 
a stand-alone detector, more complex resampling or graph-based approaches are 
considered out of scope.\\


\subsubsection{Amount of Models}
Since we have multiple different base classifiers, as well as multiple 
evaluation 
criteria, we decided to train multiple models with different purposes to see if 
any of 
them display particular aptitude for each task. This means that $8\cdot8=64$ 
different \glspl{mm} will be trained, due to there being eight different base 
classifiers, when excluding the \glspl{nn}, and eight different criteria. For the 
\gls{ee} $8\cdot8\cdot10=640$ will be trained, due to there being ten different 
employee train-test splits, and $8\cdot8\cdot9=576$ for the \gls{ce}, as there 
are nine categories for train-test splits.\\

\subsubsection{Ensembles}
In this experiment, we aim to find out if splitting the data based on some 
attribute and training an ensemble of multiple smaller models can help increase 
the scores of various performance metrics, compared to a monolithic approach. 
Ensemble models were originally inspired by how a team of people, each an 
expert in a different field, would achieve better results by making individual 
decisions and voting to decide on a final answer. As mentioned earlier we found 
two appropriate divisions of the data, one based on the type of alarm, and one 
based on the employee who handled it.   

In terms of voting, both a regular majority vote, where the answer with most 
votes wins, and a vote where at least one positive label occurs were tested. 
The first option is standard and would usually lead to higher accuracy, the 
second one provided bias towards the underrepresented class. However, both 
voting types achieved similar results and ultimately regular majority vote was 
used in the shown results.\\

\subsubsection{Neural Network Training}
Neural architectures process each customer's alarm history and produce a scalar
logit via a final linear layer. Applying the sigmoid function to this logit
yields a value $p \in [0, 1]$, which we interpret as the escalation probability,
i.e., the likelihood that the alarm should be escalated to the \textit{ADDED}
class.\\
We classify an alarm by comparing $p$ against a decision threshold $\tau$: if
$p \geq \tau$, we label the alarm as \textit{ADDED}, otherwise as
\textit{CLOSED}. During training, we fix $\tau = 0.5$, but during evaluation
we treat $\tau$ as a hyperparameter and tune it within $[0.5, 0.95]$ to trade
recall for precision.\\
We initially optimize using standard \gls{bce}:
$$
\mathcal{L}_{\texttt{BCE}} = -[y \log{(p)} + (1 - y)\log{(1 - p)}].
$$
Given the class imbalance in our dataset ($87.37\%$ \textit{CLOSED} vs
$12.63\%$ \textit{ADDED}), standard \gls{bce} risks being dominated by the
majority class, causing the model to optimize primarily for correctly
classifying non-fraudulent cases while neglecting the minority class. As
noted by Ruchay et al.~\cite{ruchay2023imbalanced}, this is problematic in
\gls{aml} contexts, where missing true fraudulent activity carries regulatory
consequences.

To address this, we also train with focal loss as introduced by Lin et
al.~\cite{lin2018focallossdenseobject}, which uses a modulating factor
$(1 - p_t)^{\gamma}$ to down-weight well-classified examples:
$$
\mathcal{L}_{\texttt{focal}} = - \alpha (1 - p_t)^{\gamma} \log{(p_t)}.
$$
Here, $p_t$ denotes the predicted probability for the ground-truth class,
$\alpha$ is a class-balancing weight, and $\gamma \geq 0$ is a focusing
parameter. When an example is easily classified ($p_t \rightarrow 1$), the
modulating factor approaches zero and the loss contribution becomes negligible.
Conversely, for hard or misclassified examples ($p_t \rightarrow 0$), the loss
remains significant. This mechanism allows the model to focus training on hard
examples, predominantly the minority \textit{ADDED} class, without requiring
explicit resampling techniques such as \gls{smote}.

Focal loss suits our recall-oriented objective: by reducing the gradient
contribution from the abundant \textit{CLOSED} examples, the model can achieve
high recall on \textit{ADDED} alarms at lower decision thresholds, ensuring that
potentially fraudulent cases are surfaced for human review.\\

\subsubsection{Voting Classifier}
In this experiment we look into utilizing a voting classifier to improve 
performance. While a \gls{vc} is similar to an ensemble in principle, it is 
utilized quite differently in this implementation. While the ensemble 
components were trained on partitioned data, the ensemble as a whole is trained 
on the complete data. The \gls{vc} is treated as an ensemble of completed 
models, each trained on their own complete data. In our 
experiments, the \gls{vc} acts as an ensemble of ensembles. However, the 
\gls{vc} has not used the ensembles exclusively. It has also utilized the 
monolithic models, so while such a description is intuitive, it is not entirely 
true.

To achieve fair testing of the model, we save the original monolith base 
models, and the \textit{X\_train} and \textit{y\_train} values. Since all 
models are trained on the same data split it ensures none of them have seen the 
data in the training split and the final test would remain fair. To bypass the 
limitation of the library we inject the pre-trained models and skip the fitting 
step. This saved on duplicate training time and ensured accurate comparisons.

Another benefit of reusing saved models was deciding on the models we 
wanted to combine within the \textit{VotingClassifier}. A good approach 
would have been to simply take the best performing models and use them, but 
since we had the framework in place already we were able to test all 
possible unique combinations. Furthermore, to preserve the hyperparameter 
optimization, the combinations included only models optimized for the same 
metric, henceforth called the 'scorer'. Utilizing identically optimized 
component models would also suggest that the \gls{vc} as a whole would be 
optimized for that specific metric. 
Additionally, to keep testing time reasonable, only 3 models were used per 
voting classifier. Since the initial results were already promising, and 
diminishing returns become more likely with additional models, we did not 
further explore this amount of classifiers.\\

\subsubsection{AutoGluon}\label{sec:autogluon}
AutoGluon was trained and evaluated as a separate experimental track from
our scikit-learn models. AutoGluon is a framework for supervised learning on
tabular data that trains multiple candidate models and can optionally improve
performance through internal ensembling. Notably, AutoGluonâ€™s reference pipeline
(v1.4, \texttt{extreme} preset, 4-hour budget) is currently ranked \#1 by Elo on the
TabArena public leaderboard across the full 51-dataset IID suite.\footnote{\url{https://huggingface.co/spaces/TabArena/leaderboard}}
We use it as a strong automated benchmark for our alarm classification task.\\
